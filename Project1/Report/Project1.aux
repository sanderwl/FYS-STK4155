\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\newlabel{eq:Franke}{{1}{4}{}{equation.0.1}{}}
\newlabel{eq:LinReg}{{2}{4}{}{equation.0.2}{}}
\newlabel{eq:minBeta}{{3}{4}{}{equation.0.3}{}}
\newlabel{eq:diagCOV}{{4}{4}{}{equation.0.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:betaIntALL1}{{1}{5}{$\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  A slice of the $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:betaIntROW1}{{2}{6}{A slice of the $\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The real Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:FrankeReal1}{{3}{7}{The real Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:FrankeEst1}{{4}{7}{The estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The difference between the real and estimated Franke functions when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:FrankeDIFF1}{{5}{8}{The difference between the real and estimated Franke functions when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.5}{}}
\newlabel{eq:MSE}{{5}{8}{}{equation.0.5}{}}
\newlabel{eq:R2}{{6}{8}{}{equation.0.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{8}{table.caption.6}\protected@file@percent }
\newlabel{tab:ESTREAL1}{{1}{8}{MSE and $R^2$ between the real and estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The real Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:FrankeReal2}{{6}{9}{The real Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:FrankeEst2}{{7}{9}{The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The difference between the real and estimated Franke functions when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:FrankeDIFF2}{{8}{10}{The difference between the real and estimated Franke functions when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:ESTREAL2}{{2}{10}{MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig:betaIntALL2}{{9}{11}{$\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:FrankeEst3}{{10}{11}{The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }}{12}{table.caption.13}\protected@file@percent }
\newlabel{tab:ESTREAL3}{{3}{12}{MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Different values of train/test MSE/$R^2$ using 100 observations, a noise-level of $0.001$, scaled data and a $75/25$ train test split.\relax }}{12}{table.caption.14}\protected@file@percent }
\newlabel{tab:asp}{{4}{12}{Different values of train/test MSE/$R^2$ using 100 observations, a noise-level of $0.001$, scaled data and a $75/25$ train test split.\relax }{table.caption.14}{}}
\newlabel{eq:mseDerive}{{7}{13}{}{equation.0.7}{}}
\newlabel{eq:mseDerive2}{{8}{13}{}{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Illustration of the bootstrap process. The statistic in question here is the MSE.\relax }}{14}{figure.caption.15}\protected@file@percent }
\newlabel{fig:Bootsketch}{{11}{14}{Illustration of the bootstrap process. The statistic in question here is the MSE.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{15}{figure.caption.16}\protected@file@percent }
\newlabel{fig:MSEBOOT1}{{12}{15}{MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSEBOOT1}. The bias and variance change as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{15}{figure.caption.17}\protected@file@percent }
\newlabel{fig:BVBOOT1}{{13}{15}{Bias and variance decomposition of figure \ref {fig:MSEBOOT1}. The bias and variance change as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 10 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{16}{figure.caption.18}\protected@file@percent }
\newlabel{fig:MSEBOOT2}{{14}{16}{MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 10 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  A rough sketch of the LOOCV process. The n observations acts as the test set once (purple squares) while the other $n-1$ observations acts as the training set (white squares). The model is then fit on each distinct permutation and a statistic is calculated (MSE in this project). Finally, an average of the k different statistics is calculated and will represent the statistic of the original data set.\relax }}{17}{figure.caption.19}\protected@file@percent }
\newlabel{fig:CVsketch}{{15}{17}{A rough sketch of the LOOCV process. The n observations acts as the test set once (purple squares) while the other $n-1$ observations acts as the training set (white squares). The model is then fit on each distinct permutation and a statistic is calculated (MSE in this project). Finally, an average of the k different statistics is calculated and will represent the statistic of the original data set.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the 10-fold cross-validation resampling method. Here we have 100 observations while we split the data into 10 different folds with a noise level of $0.001$.\relax }}{18}{figure.caption.20}\protected@file@percent }
\newlabel{fig:MSECV1}{{16}{18}{MSE as a function of polynomial degree up to 10 using the 10-fold cross-validation resampling method. Here we have 100 observations while we split the data into 10 different folds with a noise level of $0.001$.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. Here we have 100 observations while we use 10-fold CV with a noise level of $0.001$.\relax }}{18}{figure.caption.21}\protected@file@percent }
\newlabel{fig:BVCV1}{{17}{18}{Bias and variance decomposition of figure \ref {fig:MSECV1}. Here we have 100 observations while we use 10-fold CV with a noise level of $0.001$.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the 5-fold cross-validation resampling method. Here we have 100 observations while we split the data into 5 different folds with a noise level of $0.001$.\relax }}{19}{figure.caption.22}\protected@file@percent }
\newlabel{fig:MSECV2}{{18}{19}{MSE as a function of polynomial degree up to 10 using the 5-fold cross-validation resampling method. Here we have 100 observations while we split the data into 5 different folds with a noise level of $0.001$.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree. Here we have 100 observations while we use 5-fold CV with a noise level of $0.001$.\relax }}{19}{figure.caption.23}\protected@file@percent }
\newlabel{fig:BVCV2}{{19}{19}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree. Here we have 100 observations while we use 5-fold CV with a noise level of $0.001$.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the LOOCV resampling method. Here we have 100 observations while we split the data into n different folds with a noise level of $0.001$.\relax }}{20}{figure.caption.24}\protected@file@percent }
\newlabel{fig:MSECV3}{{20}{20}{MSE as a function of polynomial degree up to 10 using the LOOCV resampling method. Here we have 100 observations while we split the data into n different folds with a noise level of $0.001$.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. Here we have 100 observations while we use LOOCV with a noise level of $0.001$.\relax }}{20}{figure.caption.25}\protected@file@percent }
\newlabel{fig:BVCV3}{{21}{20}{Bias and variance decomposition of figure \ref {fig:MSECV1}. Here we have 100 observations while we use LOOCV with a noise level of $0.001$.\relax }{figure.caption.25}{}}
\newlabel{eq:RidgeDerive}{{9}{22}{}{equation.0.9}{}}
\newlabel{eq:RidgeDerive2}{{10}{22}{}{equation.0.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the bootstrap resampling method with 100 iterations. Here we have 100 observations while we split the data into $75/25$ train/test split with a noise level of $0.001$.\relax }}{23}{figure.caption.26}\protected@file@percent }
\newlabel{fig:MSERidgeBoot3}{{22}{23}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the bootstrap resampling method with 100 iterations. Here we have 100 observations while we split the data into $75/25$ train/test split with a noise level of $0.001$.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeBoot3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the bootstrap resampling method. Here we have 100 observations while we use a $75/25$ train/test split with a noise level of $0.001$.\relax }}{24}{figure.caption.27}\protected@file@percent }
\newlabel{fig:BVRidge1}{{23}{24}{Bias and variance decomposition of figure \ref {fig:MSERidgeBoot3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the bootstrap resampling method. Here we have 100 observations while we use a $75/25$ train/test split with a noise level of $0.001$.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100.\relax }}{25}{figure.caption.28}\protected@file@percent }
\newlabel{fig:LambdaPlot1}{{24}{25}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }}{25}{figure.caption.29}\protected@file@percent }
\newlabel{fig:LambdaPlot2}{{25}{25}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation with a noise level of $0.001$.\relax }}{26}{figure.caption.30}\protected@file@percent }
\newlabel{fig:MSERidgeCV1}{{26}{26}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation with a noise level of $0.001$.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 5-fold cross-validation with a noise level of $0.001$.\relax }}{27}{figure.caption.31}\protected@file@percent }
\newlabel{fig:MSERidgeCV2}{{27}{27}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 5-fold cross-validation with a noise level of $0.001$.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the leave-one-out CV resampling method. Here we have 100 observations while we consider every possible permutation of a leave-one-out cross-validation with a noise level of $0.001$.\relax }}{27}{figure.caption.32}\protected@file@percent }
\newlabel{fig:MSERidgeCV3}{{28}{27}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the leave-one-out CV resampling method. Here we have 100 observations while we consider every possible permutation of a leave-one-out cross-validation with a noise level of $0.001$.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeCV1}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }}{28}{figure.caption.33}\protected@file@percent }
\newlabel{fig:BVRidgeCV1}{{29}{28}{Bias and variance decomposition of figure \ref {fig:MSERidgeCV1}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeCV2}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }}{29}{figure.caption.34}\protected@file@percent }
\newlabel{fig:BVRidgeCV2}{{30}{29}{Bias and variance decomposition of figure \ref {fig:MSERidgeCV2}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeCV3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the LOOCV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }}{29}{figure.caption.35}\protected@file@percent }
\newlabel{fig:BVRidgeCV3}{{31}{29}{Bias and variance decomposition of figure \ref {fig:MSERidgeCV3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the LOOCV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 10-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }}{30}{figure.caption.36}\protected@file@percent }
\newlabel{fig:LambdaPlotCV1}{{32}{30}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 10-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 5-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }}{30}{figure.caption.37}\protected@file@percent }
\newlabel{fig:LambdaPlotCV2}{{33}{30}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 5-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the LOOCV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }}{31}{figure.caption.38}\protected@file@percent }
\newlabel{fig:LambdaPlotCV3}{{34}{31}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the LOOCV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }{figure.caption.38}{}}
\newlabel{eq:LassoDerive2}{{11}{32}{}{equation.0.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the bootstrap resampling method and a Lasso regression scheme. Here we have $100 \times 100$ observations randomly drawn from the original data with a noise level of $0.001$.\relax }}{32}{figure.caption.39}\protected@file@percent }
\newlabel{fig:MSELasso1}{{35}{32}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the bootstrap resampling method and a Lasso regression scheme. Here we have $100 \times 100$ observations randomly drawn from the original data with a noise level of $0.001$.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 10-fold CV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation with a noise level of $0.001$.\relax }}{33}{figure.caption.40}\protected@file@percent }
\newlabel{fig:MSELasso2}{{36}{33}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 10-fold CV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation with a noise level of $0.001$.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 5-fold CV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a 5-fold cross-validation with a noise level of $0.001$.\relax }}{33}{figure.caption.41}\protected@file@percent }
\newlabel{fig:MSELasso3}{{37}{33}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 5-fold CV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a 5-fold cross-validation with a noise level of $0.001$.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the LOOCV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a LOOCV with a noise level of $0.001$.\relax }}{34}{figure.caption.42}\protected@file@percent }
\newlabel{fig:MSELasso4}{{38}{34}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the LOOCV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a LOOCV with a noise level of $0.001$.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }}{35}{figure.caption.43}\protected@file@percent }
\newlabel{fig:LambdaLasso1}{{39}{35}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 10-fold CV resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }}{35}{figure.caption.44}\protected@file@percent }
\newlabel{fig:LambdaLasso2}{{40}{35}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 10-fold CV resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 5-fold CV resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }}{36}{figure.caption.45}\protected@file@percent }
\newlabel{fig:LambdaLasso3}{{41}{36}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 5-fold CV resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the LOOCV resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }}{36}{figure.caption.46}\protected@file@percent }
\newlabel{fig:LambdaLasso4}{{42}{36}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the LOOCV resampling method. The original dataset is of size 100 with a noise-level of $0.001$. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces  Terrain data over Norway.\relax }}{38}{figure.caption.47}\protected@file@percent }
\newlabel{fig:terrainData1}{{43}{38}{Terrain data over Norway.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces  Terrain data over Norway where the data has been scaled and reduced to $\frac  {n}{10}$ data points.\relax }}{39}{figure.caption.48}\protected@file@percent }
\newlabel{fig:terrainData2}{{44}{39}{Terrain data over Norway where the data has been scaled and reduced to $\frac {n}{10}$ data points.\relax }{figure.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  An overview of the three models which will be updated as we go. Undiscovered parameters/values are denoted by "-".\relax }}{40}{table.caption.49}\protected@file@percent }
\newlabel{tab:update1}{{5}{40}{An overview of the three models which will be updated as we go. Undiscovered parameters/values are denoted by "-".\relax }{table.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces  MSE as a function of polynomial degree up to 20 using the 10-fold CV resampling method and a OLS regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation.\relax }}{41}{figure.caption.50}\protected@file@percent }
\newlabel{fig:MSEOLST}{{45}{41}{MSE as a function of polynomial degree up to 20 using the 10-fold CV resampling method and a OLS regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces  MSE as a function of polynomial degree up to 20 for different values of $\lambda $ using the 10-fold CV resampling method and a Ridge regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation.\relax }}{41}{figure.caption.51}\protected@file@percent }
\newlabel{fig:MSERIDGET}{{46}{41}{MSE as a function of polynomial degree up to 20 for different values of $\lambda $ using the 10-fold CV resampling method and a Ridge regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces  MSE as a function of polynomial degree up to 20 for different values of $\lambda $ using the 10-fold CV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation.\relax }}{42}{figure.caption.52}\protected@file@percent }
\newlabel{fig:MSELASSOT}{{47}{42}{MSE as a function of polynomial degree up to 20 for different values of $\lambda $ using the 10-fold CV resampling method and a Lasso regression scheme. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation.\relax }{figure.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  An overview of the three models which will be updated as we go. Undiscovered parameters/values are denoted by "-".\relax }}{42}{table.caption.53}\protected@file@percent }
\newlabel{tab:update2}{{6}{42}{An overview of the three models which will be updated as we go. Undiscovered parameters/values are denoted by "-".\relax }{table.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces  Final image of the test data using the OLS model based on the optimal parameters in table \ref  {tab:update2}.\relax }}{43}{figure.caption.54}\protected@file@percent }
\newlabel{fig:predOLST}{{48}{43}{Final image of the test data using the OLS model based on the optimal parameters in table \ref {tab:update2}.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces  Final image of the test data using the Ridge model based on the optimal parameters in table \ref  {tab:update2}.\relax }}{43}{figure.caption.55}\protected@file@percent }
\newlabel{fig:predRIDGET}{{49}{43}{Final image of the test data using the Ridge model based on the optimal parameters in table \ref {tab:update2}.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces  Final image of the test data using the Lasso model based on the optimal parameters in table \ref  {tab:update2}.\relax }}{44}{figure.caption.56}\protected@file@percent }
\newlabel{fig:predLASSOT}{{50}{44}{Final image of the test data using the Lasso model based on the optimal parameters in table \ref {tab:update2}.\relax }{figure.caption.56}{}}
