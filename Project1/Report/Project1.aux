\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\newlabel{eq:Franke}{{1}{3}{}{equation.0.1}{}}
\newlabel{eq:LinReg}{{2}{3}{}{equation.0.2}{}}
\newlabel{eq:minBeta}{{3}{3}{}{equation.0.3}{}}
\newlabel{eq:diagCOV}{{4}{3}{}{equation.0.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:betaIntALL1}{{1}{4}{$\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  A slice of the $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the $95\%$ confidence interval ($\pm \sigma $).\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:betaIntROW1}{{2}{4}{A slice of the $\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the $95\%$ confidence interval ($\pm \sigma $).\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The real Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:FrankeReal1}{{3}{5}{The real Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:FrankeEst1}{{4}{5}{The estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The difference between the real and estimated Franke functions when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:FrankeDIFF1}{{5}{6}{The difference between the real and estimated Franke functions when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.5}{}}
\newlabel{eq:MSE}{{5}{6}{}{equation.0.5}{}}
\newlabel{eq:R2}{{6}{6}{}{equation.0.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{6}{table.caption.6}\protected@file@percent }
\newlabel{tab:ESTREAL1}{{1}{6}{MSE and $R^2$ between the real and estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The real Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:FrankeReal2}{{6}{7}{The real Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig:FrankeEst2}{{7}{7}{The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The difference between the real and estimated Franke functions when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:FrankeDIFF2}{{8}{8}{The difference between the real and estimated Franke functions when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{8}{table.caption.10}\protected@file@percent }
\newlabel{tab:ESTREAL2}{{2}{8}{MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig:betaIntALL2}{{9}{9}{$\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{fig:FrankeEst3}{{10}{9}{The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }}{10}{table.caption.13}\protected@file@percent }
\newlabel{tab:ESTREAL3}{{3}{10}{MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }{table.caption.13}{}}
\newlabel{eq:mseDerive}{{7}{11}{}{equation.0.7}{}}
\newlabel{eq:mseDerive2}{{8}{11}{}{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Illustration of the bootstrap process. The statistic in question here is the MSE.\relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig:Bootsketch}{{11}{12}{Illustration of the bootstrap process. The statistic in question here is the MSE.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig:MSEBOOT1}{{12}{13}{MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSEBOOT1}. The bias and variance change as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:BVBOOT1}{{13}{13}{Bias and variance decomposition of figure \ref {fig:MSEBOOT1}. The bias and variance change as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 10 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{14}{figure.caption.17}\protected@file@percent }
\newlabel{fig:MSEBOOT2}{{14}{14}{MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 10 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  A rough sketch of the LOOCV process. The n observations acts as the test set once (purple squares) while the other $n-1$ observations acts as the training set (white squares). The model is then fit on each distinct permutation and a statistic is calculated (MSE in this project). Finally, an average of the k different statistics is calculated and will represent the statistic of the original data set.\relax }}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig:CVsketch}{{15}{15}{A rough sketch of the LOOCV process. The n observations acts as the test set once (purple squares) while the other $n-1$ observations acts as the training set (white squares). The model is then fit on each distinct permutation and a statistic is calculated (MSE in this project). Finally, an average of the k different statistics is calculated and will represent the statistic of the original data set.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 10 different folds with a noise level of $0.001$.\relax }}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig:MSECV1}{{16}{16}{MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 10 different folds with a noise level of $0.001$.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 10-fold CV with a noise level of $0.001$.\relax }}{16}{figure.caption.20}\protected@file@percent }
\newlabel{fig:BVCV1}{{17}{16}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 10-fold CV with a noise level of $0.001$.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 5 different folds with a noise level of $0.001$.\relax }}{17}{figure.caption.21}\protected@file@percent }
\newlabel{fig:MSECV2}{{18}{17}{MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 5 different folds with a noise level of $0.001$.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 5-fold CV with a noise level of $0.001$.\relax }}{17}{figure.caption.22}\protected@file@percent }
\newlabel{fig:BVCV2}{{19}{17}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 5-fold CV with a noise level of $0.001$.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into n different folds with a noise level of $0.001$.\relax }}{18}{figure.caption.23}\protected@file@percent }
\newlabel{fig:MSECV3}{{20}{18}{MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into n different folds with a noise level of $0.001$.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use LOOCV with a noise level of $0.001$.\relax }}{18}{figure.caption.24}\protected@file@percent }
\newlabel{fig:BVCV3}{{21}{18}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use LOOCV with a noise level of $0.001$.\relax }{figure.caption.24}{}}
\newlabel{eq:RidgeDerive}{{9}{20}{}{equation.0.9}{}}
\newlabel{eq:RidgeDerive2}{{10}{20}{}{equation.0.10}{}}
