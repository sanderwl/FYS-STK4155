\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\newlabel{eq:Franke}{{1}{3}{}{equation.0.1}{}}
\newlabel{eq:LinReg}{{2}{3}{}{equation.0.2}{}}
\newlabel{eq:minBeta}{{3}{3}{}{equation.0.3}{}}
\newlabel{eq:diagCOV}{{4}{3}{}{equation.0.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:betaIntALL1}{{1}{4}{$\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  A slice of the $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the $95\%$ confidence interval ($\pm \sigma $).\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:betaIntROW1}{{2}{4}{A slice of the $\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the $95\%$ confidence interval ($\pm \sigma $).\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The real Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:FrankeReal1}{{3}{5}{The real Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:FrankeEst1}{{4}{5}{The estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The difference between the real and estimated Franke functions when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:FrankeDIFF1}{{5}{6}{The difference between the real and estimated Franke functions when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.5}{}}
\newlabel{eq:MSE}{{5}{6}{}{equation.0.5}{}}
\newlabel{eq:R2}{{6}{6}{}{equation.0.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{6}{table.caption.6}\protected@file@percent }
\newlabel{tab:ESTREAL1}{{1}{6}{MSE and $R^2$ between the real and estimated Franke function when we have $10$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The real Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:FrankeReal2}{{6}{7}{The real Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig:FrankeEst2}{{7}{7}{The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The difference between the real and estimated Franke functions when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:FrankeDIFF2}{{8}{8}{The difference between the real and estimated Franke functions when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }}{8}{table.caption.10}\protected@file@percent }
\newlabel{tab:ESTREAL2}{{2}{8}{MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.001$ and a $75/25$ train/test split.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  $\hat  {\bm  {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat  {\bm  {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig:betaIntALL2}{{9}{9}{$\hat {\boldsymbol {\beta }}$-coefficients from performing ordinary least squares regression. Dots indicate the actual $\hat {\boldsymbol {\beta }}$-coefficients value while bars around indicate the confidence interval ($\pm \sigma $).\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{fig:FrankeEst3}{{10}{9}{The estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }}{10}{table.caption.13}\protected@file@percent }
\newlabel{tab:ESTREAL3}{{3}{10}{MSE and $R^2$ between the real and estimated Franke function when we have $100$ observations and polynomials of degree $5$ using a noise-level of $0.1$ and a $75/25$ train/test split.\relax }{table.caption.13}{}}
\newlabel{eq:mseDerive}{{7}{11}{}{equation.0.7}{}}
\newlabel{eq:mseDerive2}{{8}{11}{}{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Illustration of the bootstrap process. The statistic in question here is the MSE.\relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig:Bootsketch}{{11}{12}{Illustration of the bootstrap process. The statistic in question here is the MSE.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig:MSEBOOT1}{{12}{13}{MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSEBOOT1}. The bias and variance change as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:BVBOOT1}{{13}{13}{Bias and variance decomposition of figure \ref {fig:MSEBOOT1}. The bias and variance change as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 100 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 10 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }}{14}{figure.caption.17}\protected@file@percent }
\newlabel{fig:MSEBOOT2}{{14}{14}{MSE as a function of polynomial degree using the bootstrap resampling method. Here we have 100 observations while we generate 10 bootstrap samples with a noise level of $0.001$ and a $75/25$ train test split.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  A rough sketch of the LOOCV process. The n observations acts as the test set once (purple squares) while the other $n-1$ observations acts as the training set (white squares). The model is then fit on each distinct permutation and a statistic is calculated (MSE in this project). Finally, an average of the k different statistics is calculated and will represent the statistic of the original data set.\relax }}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig:CVsketch}{{15}{15}{A rough sketch of the LOOCV process. The n observations acts as the test set once (purple squares) while the other $n-1$ observations acts as the training set (white squares). The model is then fit on each distinct permutation and a statistic is calculated (MSE in this project). Finally, an average of the k different statistics is calculated and will represent the statistic of the original data set.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 10 different folds with a noise level of $0.001$.\relax }}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig:MSECV1}{{16}{16}{MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 10 different folds with a noise level of $0.001$.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 10-fold CV with a noise level of $0.001$.\relax }}{16}{figure.caption.20}\protected@file@percent }
\newlabel{fig:BVCV1}{{17}{16}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 10-fold CV with a noise level of $0.001$.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 5 different folds with a noise level of $0.001$.\relax }}{17}{figure.caption.21}\protected@file@percent }
\newlabel{fig:MSECV2}{{18}{17}{MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into 5 different folds with a noise level of $0.001$.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 5-fold CV with a noise level of $0.001$.\relax }}{17}{figure.caption.22}\protected@file@percent }
\newlabel{fig:BVCV2}{{19}{17}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use 5-fold CV with a noise level of $0.001$.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into n different folds with a noise level of $0.001$.\relax }}{18}{figure.caption.23}\protected@file@percent }
\newlabel{fig:MSECV3}{{20}{18}{MSE as a function of polynomial degree up to 10 using the cross-validation resampling method. Here we have 100 observations while we split the data into n different folds with a noise level of $0.001$.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use LOOCV with a noise level of $0.001$.\relax }}{18}{figure.caption.24}\protected@file@percent }
\newlabel{fig:BVCV3}{{21}{18}{Bias and variance decomposition of figure \ref {fig:MSECV1}. The bias and variance change as a function of polynomial degree using the CV resampling method. Here we have 100 observations while we use LOOCV with a noise level of $0.001$.\relax }{figure.caption.24}{}}
\newlabel{eq:RidgeDerive}{{9}{20}{}{equation.0.9}{}}
\newlabel{eq:RidgeDerive2}{{10}{20}{}{equation.0.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the bootstrap resampling method with 100 iterations. Here we have 100 observations while we split the data into $75/25$ train/test split with a noise level of $0.001$.\relax }}{20}{figure.caption.25}\protected@file@percent }
\newlabel{fig:MSERidgeBoot3}{{22}{20}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the bootstrap resampling method with 100 iterations. Here we have 100 observations while we split the data into $75/25$ train/test split with a noise level of $0.001$.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeBoot3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the bootstrap resampling method. Here we have 100 observations while we use a $75/25$ train/test split with a noise level of $0.001$.\relax }}{21}{figure.caption.26}\protected@file@percent }
\newlabel{fig:BVRidge1}{{23}{21}{Bias and variance decomposition of figure \ref {fig:MSERidgeBoot3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the bootstrap resampling method. Here we have 100 observations while we use a $75/25$ train/test split with a noise level of $0.001$.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100.\relax }}{22}{figure.caption.27}\protected@file@percent }
\newlabel{fig:LambdaPlot1}{{24}{22}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }}{22}{figure.caption.28}\protected@file@percent }
\newlabel{fig:LambdaPlot2}{{25}{22}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the bootstrap method with 100 iterations. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation with a noise level of $0.001$.\relax }}{23}{figure.caption.29}\protected@file@percent }
\newlabel{fig:MSERidgeCV1}{{26}{23}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 10-fold cross-validation with a noise level of $0.001$.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 5-fold cross-validation with a noise level of $0.001$.\relax }}{24}{figure.caption.30}\protected@file@percent }
\newlabel{fig:MSERidgeCV2}{{27}{24}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations while we consider every possible permutation of a 5-fold cross-validation with a noise level of $0.001$.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces  MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the leave-one-out CV resampling method. Here we have 100 observations while we consider every possible permutation of a leave-one-out cross-validation with a noise level of $0.001$.\relax }}{24}{figure.caption.31}\protected@file@percent }
\newlabel{fig:MSERidgeCV3}{{28}{24}{MSE as a function of polynomial degree up to 10 for different values of $\lambda $ using the leave-one-out CV resampling method. Here we have 100 observations while we consider every possible permutation of a leave-one-out cross-validation with a noise level of $0.001$.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeCV1}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }}{25}{figure.caption.32}\protected@file@percent }
\newlabel{fig:BVRidgeCV1}{{29}{25}{Bias and variance decomposition of figure \ref {fig:MSERidgeCV1}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 10-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeCV2}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }}{26}{figure.caption.33}\protected@file@percent }
\newlabel{fig:BVRidgeCV2}{{30}{26}{Bias and variance decomposition of figure \ref {fig:MSERidgeCV2}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the 5-fold CV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces  Bias and variance decomposition of figure \ref  {fig:MSERidgeCV3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the LOOCV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }}{26}{figure.caption.34}\protected@file@percent }
\newlabel{fig:BVRidgeCV3}{{31}{26}{Bias and variance decomposition of figure \ref {fig:MSERidgeCV3}. The bias and variance change as a function of polynomial degree and values of $\lambda $ using the LOOCV resampling method. Here we have 100 observations with a noise level of $0.001$.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 10-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }}{27}{figure.caption.35}\protected@file@percent }
\newlabel{fig:LambdaPlotCV1}{{32}{27}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 10-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 5-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }}{27}{figure.caption.36}\protected@file@percent }
\newlabel{fig:LambdaPlotCV2}{{33}{27}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the 5-fold CV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces  Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the LOOCV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }}{28}{figure.caption.37}\protected@file@percent }
\newlabel{fig:LambdaPlotCV3}{{34}{28}{Regression coefficient values of up to $p = 5$ as function of logarithmic $\lambda $-value using the LOOCV method. The original dataset is of size 100. Here we have sorted polynomials of the same degree into the same color\relax }{figure.caption.37}{}}
