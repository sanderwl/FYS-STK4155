\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\newlabel{eq:MSEdef}{{1}{4}{}{equation.0.1}{}}
\newlabel{eq:MSEder}{{2}{4}{}{equation.0.2}{}}
\newlabel{eq:scheduleTIME}{{3}{5}{}{equation.0.3}{}}
\newlabel{eq:MSEderSTO}{{4}{5}{}{equation.0.4}{}}
\newlabel{eq:MSEderSTOridge}{{5}{5}{}{equation.0.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  An overview of standard parameters to be used in the analysis.\relax }}{6}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:standardParam}{{1}{6}{An overview of standard parameters to be used in the analysis.\relax }{table.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The MSE as function of number of epoch iterations using static learning rate and OLS equivalent SGD.\relax }}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandard}{{1}{6}{The MSE as function of number of epoch iterations using static learning rate and OLS equivalent SGD.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The MSE as function of number of epoch iterations using static learning rate and Ridge equivalent SGD.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandardRidge}{{2}{7}{The MSE as function of number of epoch iterations using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using static learning rate and OLS equivalent SGD.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchOLS}{{3}{7}{The MSE as function of number of epoch iterations and batch sizes using static learning rate and OLS equivalent SGD.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using static learning rate and Ridge equivalent SGD.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchRIDGE}{{4}{8}{The MSE as function of number of epoch iterations and batch sizes using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The MSE as function of number of epoch iterations and learning rates using static learning rate and OLS equivalent SGD.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlaernOLS}{{5}{9}{The MSE as function of number of epoch iterations and learning rates using static learning rate and OLS equivalent SGD.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The MSE as function of number of epoch iterations and learning rates using static learning rate and Ridge equivalent SGD.\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlearnRIDGE}{{6}{9}{The MSE as function of number of epoch iterations and learning rates using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using static learning rate and Ridge equivalent SGD.\relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlambRIDGE}{{7}{10}{The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The MSE as function of number of epoch iterations using a learning schedule rate and OLS equivalent SGD.\relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandard_sch}{{8}{11}{The MSE as function of number of epoch iterations using a learning schedule rate and OLS equivalent SGD.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  The MSE as function of number of epoch iterations using a learning schedule rate and Ridge equivalent SGD.\relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandardRidge_sch}{{9}{11}{The MSE as function of number of epoch iterations using a learning schedule rate and Ridge equivalent SGD.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using a learning schedule and OLS equivalent SGD.\relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchOLS_sch}{{10}{12}{The MSE as function of number of epoch iterations and batch sizes using a learning schedule and OLS equivalent SGD.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using a learning schedule and Ridge equivalent SGD.\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchRIDGE_sch}{{11}{12}{The MSE as function of number of epoch iterations and batch sizes using a learning schedule and Ridge equivalent SGD.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD.\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlambRIDGE_sch}{{12}{13}{The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD. This is a zoom-in of figure \ref  {fig:MSEvsEPOCHlambRIDGE_sch}\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlambRIDGE_sch2}{{13}{14}{The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD. This is a zoom-in of figure \ref {fig:MSEvsEPOCHlambRIDGE_sch}\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  Conseptual drawing of a neural network. Here we have one input layer, two hidden layers and one output layer. The input and output layers consist of a single neuron, while the hidden layers have three neurons each. The input to each neuron is a weighted sum (W) plus a bias (B) from the previous layer. The output of a given neuron is the input to that neuron going through a activation function $\sigma $ which is then passed to the neurons in the next layer. The output activation function may be different than that of the neuron activation functions.\relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig:nnconcept}{{14}{15}{Conseptual drawing of a neural network. Here we have one input layer, two hidden layers and one output layer. The input and output layers consist of a single neuron, while the hidden layers have three neurons each. The input to each neuron is a weighted sum (W) plus a bias (B) from the previous layer. The output of a given neuron is the input to that neuron going through a activation function $\sigma $ which is then passed to the neurons in the next layer. The output activation function may be different than that of the neuron activation functions.\relax }{figure.caption.15}{}}
\newlabel{eq:sig}{{6}{16}{}{equation.0.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  The MSE as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the OLS implementation.\relax }}{17}{figure.caption.16}\protected@file@percent }
\newlabel{fig:MSEvsLearn_OLS}{{15}{17}{The MSE as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the OLS implementation.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  The MSE as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the Ridge implementation.\relax }}{17}{figure.caption.17}\protected@file@percent }
\newlabel{fig:MSEvsLearn_Ridge}{{16}{17}{The MSE as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the Ridge implementation.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  The $R^2$ as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the OLS implementation.\relax }}{18}{figure.caption.18}\protected@file@percent }
\newlabel{fig:R2vsLearn_OLS}{{17}{18}{The $R^2$ as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the OLS implementation.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  The $R^2$ as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the Ridge implementation.\relax }}{18}{figure.caption.19}\protected@file@percent }
\newlabel{fig:R2vsLearn_Ridge}{{18}{18}{The $R^2$ as function of different learning rates for the SGD regression scheme, the neural network implementation and the Scikit neural network implementation. Here we have utilized the Ridge implementation.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Heatmap showing the MSE for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 100 neurons and 14 layers indicate the area of lowest MSE. Here we have utilized the OLS implementation.\relax }}{19}{figure.caption.20}\protected@file@percent }
\newlabel{fig:heatOLS}{{19}{19}{Heatmap showing the MSE for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 100 neurons and 14 layers indicate the area of lowest MSE. Here we have utilized the OLS implementation.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Heatmap showing the $R^2$ for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 100 neurons and 14 layers indicate the area of highest $R^2$. Here we have utilized the OLS implementation.\relax }}{20}{figure.caption.21}\protected@file@percent }
\newlabel{fig:heatOLSR2}{{20}{20}{Heatmap showing the $R^2$ for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 100 neurons and 14 layers indicate the area of highest $R^2$. Here we have utilized the OLS implementation.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Heatmap showing the MSE using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 180 neurons and 9 layers indicate the area of lowest MSE. Here we have utilized the OLS implementation.\relax }}{20}{figure.caption.22}\protected@file@percent }
\newlabel{fig:heatOLSsci}{{21}{20}{Heatmap showing the MSE using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 180 neurons and 9 layers indicate the area of lowest MSE. Here we have utilized the OLS implementation.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Heatmap showing the $R^2$ using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 180 neurons and 9 layers indicate the area of highest $R^2$. Here we have utilized the OLS implementation.\relax }}{21}{figure.caption.23}\protected@file@percent }
\newlabel{fig:heatOLSsciR2}{{22}{21}{Heatmap showing the $R^2$ using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 180 neurons and 9 layers indicate the area of highest $R^2$. Here we have utilized the OLS implementation.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Heatmap showing the MSE for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 160 neurons and 14 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }}{21}{figure.caption.24}\protected@file@percent }
\newlabel{fig:heatRidge}{{23}{21}{Heatmap showing the MSE for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 160 neurons and 14 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Heatmap showing the $R^2$ for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 160 neurons and 14 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }}{22}{figure.caption.25}\protected@file@percent }
\newlabel{fig:heatRidgeR2}{{24}{22}{Heatmap showing the $R^2$ for my own neural network for different numbers of hidden layers and hidden neurons. Blue square at 160 neurons and 14 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Heatmap showing the MSE using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 190 neurons and 15 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }}{22}{figure.caption.26}\protected@file@percent }
\newlabel{fig:heatRidgesci}{{25}{22}{Heatmap showing the MSE using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 190 neurons and 15 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  Heatmap showing the $R^2$ using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 190 neurons and 3 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }}{23}{figure.caption.27}\protected@file@percent }
\newlabel{fig:heatRidgesciR2}{{26}{23}{Heatmap showing the $R^2$ using Scikit implementation for different numbers of hidden layers and hidden neurons. Blue square at 190 neurons and 3 layers indicate the area of lowest MSE. Here we have utilized the Ridge implementation.\relax }{figure.caption.27}{}}
\newlabel{eq:RELU}{{7}{24}{}{equation.0.7}{}}
\newlabel{eq:leakyRELU}{{8}{24}{}{equation.0.8}{}}
