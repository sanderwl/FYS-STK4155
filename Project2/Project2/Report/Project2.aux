\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\newlabel{eq:Franke}{{1}{4}{}{equation.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A plot of the Franke function discretized with 100 times a 100 observations. The function is standardized and normalized such that the mean is zero and the variance is 1 and the maximum value is 1.\relax }}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Franke}{{1}{4}{A plot of the Franke function discretized with 100 times a 100 observations. The function is standardized and normalized such that the mean is zero and the variance is 1 and the maximum value is 1.\relax }{figure.caption.1}{}}
\newlabel{eq:MSEdef}{{2}{5}{}{equation.0.2}{}}
\newlabel{eq:MSEder}{{3}{5}{}{equation.0.3}{}}
\newlabel{eq:scheduleTIME}{{4}{5}{}{equation.0.4}{}}
\newlabel{eq:MSEderSTO}{{5}{6}{}{equation.0.5}{}}
\newlabel{eq:MSEderSTOridge}{{6}{6}{}{equation.0.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  An overview of standard parameters to be used in the analysis.\relax }}{7}{table.caption.2}\protected@file@percent }
\newlabel{tab:standardParam}{{1}{7}{An overview of standard parameters to be used in the analysis.\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The MSE as function of number of epoch iterations using static learning rate and OLS equivalent SGD.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandard}{{2}{7}{The MSE as function of number of epoch iterations using static learning rate and OLS equivalent SGD.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The MSE as function of number of epoch iterations using static learning rate and Ridge equivalent SGD.\relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandardRidge}{{3}{8}{The MSE as function of number of epoch iterations using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using static learning rate and OLS equivalent SGD.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchOLS}{{4}{8}{The MSE as function of number of epoch iterations and batch sizes using static learning rate and OLS equivalent SGD.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using static learning rate and Ridge equivalent SGD.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchRIDGE}{{5}{9}{The MSE as function of number of epoch iterations and batch sizes using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The MSE as function of number of epoch iterations and learning rates using static learning rate and OLS equivalent SGD.\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlaernOLS}{{6}{10}{The MSE as function of number of epoch iterations and learning rates using static learning rate and OLS equivalent SGD.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The MSE as function of number of epoch iterations and learning rates using static learning rate and Ridge equivalent SGD.\relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlearnRIDGE}{{7}{10}{The MSE as function of number of epoch iterations and learning rates using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using static learning rate and Ridge equivalent SGD.\relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlambRIDGE}{{8}{11}{The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using static learning rate and Ridge equivalent SGD.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  The MSE as function of number of epoch iterations using a learning schedule rate and OLS equivalent SGD.\relax }}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandard_sch}{{9}{12}{The MSE as function of number of epoch iterations using a learning schedule rate and OLS equivalent SGD.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The MSE as function of number of epoch iterations using a learning schedule rate and Ridge equivalent SGD.\relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHstandardRidge_sch}{{10}{12}{The MSE as function of number of epoch iterations using a learning schedule rate and Ridge equivalent SGD.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using a learning schedule and OLS equivalent SGD.\relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchOLS_sch}{{11}{13}{The MSE as function of number of epoch iterations and batch sizes using a learning schedule and OLS equivalent SGD.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  The MSE as function of number of epoch iterations and batch sizes using a learning schedule and Ridge equivalent SGD.\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHbatchRIDGE_sch}{{12}{13}{The MSE as function of number of epoch iterations and batch sizes using a learning schedule and Ridge equivalent SGD.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD.\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlambRIDGE_sch}{{13}{14}{The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD. This is a zoom-in of figure \ref  {fig:MSEvsEPOCHlambRIDGE_sch}\relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig:MSEvsEPOCHlambRIDGE_sch2}{{14}{15}{The MSE as function of number of epoch iterations and penalty parameter $\lambda $ using a learning schedule and Ridge equivalent SGD. This is a zoom-in of figure \ref {fig:MSEvsEPOCHlambRIDGE_sch}\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  Conseptual drawing of a neural network. Here we have one input layer, two hidden layers and one output layer. The input and output layers consist of a single neuron, while the hidden layers have three neurons each. The input to each neuron is a weighted sum (W) plus a bias (B) from the previous layer. The output of a given neuron is the input to that neuron going through a activation function $\sigma $ which is then passed to the neurons in the next layer. The output activation function may be different than that of the neuron activation functions.\relax }}{16}{figure.caption.16}\protected@file@percent }
\newlabel{fig:nnconcept}{{15}{16}{Conseptual drawing of a neural network. Here we have one input layer, two hidden layers and one output layer. The input and output layers consist of a single neuron, while the hidden layers have three neurons each. The input to each neuron is a weighted sum (W) plus a bias (B) from the previous layer. The output of a given neuron is the input to that neuron going through a activation function $\sigma $ which is then passed to the neurons in the next layer. The output activation function may be different than that of the neuron activation functions.\relax }{figure.caption.16}{}}
\newlabel{eq:sig}{{7}{17}{}{equation.0.7}{}}
\newlabel{eq:RELU}{{8}{17}{}{equation.0.8}{}}
\newlabel{eq:leakyRELU}{{9}{17}{}{equation.0.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  The MSE as function of number of learning rate for the three activation functions when we utilize 100 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }}{19}{figure.caption.17}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL1}{{16}{19}{The MSE as function of number of learning rate for the three activation functions when we utilize 100 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  The $R^2$ as function of number of learning rate for the three activation functions when we utilize 100 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL2}{{17}{19}{The $R^2$ as function of number of learning rate for the three activation functions when we utilize 100 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  The MSE as function of number of learning rate for the three activation functions when we utilize 500 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL3}{{18}{20}{The MSE as function of number of learning rate for the three activation functions when we utilize 500 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  The $R^2$ as function of number of learning rate for the three activation functions when we utilize 500 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }}{20}{figure.caption.20}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL4}{{19}{20}{The $R^2$ as function of number of learning rate for the three activation functions when we utilize 500 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  The MSE as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }}{21}{figure.caption.21}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL5}{{20}{21}{The MSE as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  The $R^2$ as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }}{21}{figure.caption.22}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL6}{{21}{21}{The $R^2$ as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 10. This is the OLS equivalent.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  The MSE as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the OLS equivalent.\relax }}{22}{figure.caption.23}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL7}{{22}{22}{The MSE as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the OLS equivalent.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  The $R^2$ as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the OLS equivalent.\relax }}{22}{figure.caption.24}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL8}{{23}{22}{The $R^2$ as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the OLS equivalent.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the sigmoid activation function. This is the OLS equivalent.\relax }}{23}{figure.caption.25}\protected@file@percent }
\newlabel{fig:heatmap1}{{24}{23}{Heatmap for different number of hidden layers and neurons using the sigmoid activation function. This is the OLS equivalent.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the RELU activation function. This is the OLS equivalent.\relax }}{24}{figure.caption.26}\protected@file@percent }
\newlabel{fig:heatmap2}{{25}{24}{Heatmap for different number of hidden layers and neurons using the RELU activation function. This is the OLS equivalent.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the RELU activation function with the Scikit implementation. This is the OLS equivalent.\relax }}{24}{figure.caption.27}\protected@file@percent }
\newlabel{fig:heatmap3}{{26}{24}{Heatmap for different number of hidden layers and neurons using the RELU activation function with the Scikit implementation. This is the OLS equivalent.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the leaky RELU activation function with the Scikit implementation. This is the OLS equivalent.\relax }}{25}{figure.caption.28}\protected@file@percent }
\newlabel{fig:heatmap4}{{27}{25}{Heatmap for different number of hidden layers and neurons using the leaky RELU activation function with the Scikit implementation. This is the OLS equivalent.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces  The MSE as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the Ridge equivalent.\relax }}{26}{figure.caption.29}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL8}{{28}{26}{The MSE as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the Ridge equivalent.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces  The $R^2$ as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the Ridge equivalent.\relax }}{26}{figure.caption.30}\protected@file@percent }
\newlabel{fig:MSEvsLrateTOTAL9}{{29}{26}{The $R^2$ as function of number of learning rate for the three activation functions when we utilize 2000 epoch iterations and a batch size of 2. This is the Ridge equivalent.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the sigmoid activation function. This is the Ridge equivalent.\relax }}{27}{figure.caption.31}\protected@file@percent }
\newlabel{fig:heatmap5}{{30}{27}{Heatmap for different number of hidden layers and neurons using the sigmoid activation function. This is the Ridge equivalent.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the RELU activation function. This is the Ridge equivalent.\relax }}{28}{figure.caption.32}\protected@file@percent }
\newlabel{fig:heatmap6}{{31}{28}{Heatmap for different number of hidden layers and neurons using the RELU activation function. This is the Ridge equivalent.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the Scikit RELU activation function. This is the Ridge equivalent.\relax }}{28}{figure.caption.33}\protected@file@percent }
\newlabel{fig:heatmap7}{{32}{28}{Heatmap for different number of hidden layers and neurons using the Scikit RELU activation function. This is the Ridge equivalent.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces  Heatmap for different number of hidden layers and neurons using the leaky RELU activation function. This is the Ridge equivalent.\relax }}{29}{figure.caption.34}\protected@file@percent }
\newlabel{fig:heatmap8}{{33}{29}{Heatmap for different number of hidden layers and neurons using the leaky RELU activation function. This is the Ridge equivalent.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces  Example of how the training data looks. The digit shown is a hand-written digit that would be hard for a computer to recognize.\relax }}{30}{figure.caption.35}\protected@file@percent }
\newlabel{fig:DigitDataFig}{{34}{30}{Example of how the training data looks. The digit shown is a hand-written digit that would be hard for a computer to recognize.\relax }{figure.caption.35}{}}
\newlabel{eq:acc}{{10}{30}{}{equation.0.10}{}}
\newlabel{eq:softmax}{{11}{31}{}{equation.0.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces  Neural network accuracy in terms of learning rate and different activation functions.\relax }}{31}{figure.caption.36}\protected@file@percent }
\newlabel{fig:AccvsLrate}{{35}{31}{Neural network accuracy in terms of learning rate and different activation functions.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces  Accuracy for different configurations of hidden neurons and hidden layers.\relax }}{32}{figure.caption.37}\protected@file@percent }
\newlabel{fig:AccvsLrate2}{{36}{32}{Accuracy for different configurations of hidden neurons and hidden layers.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces  Accuracy for different configurations of hidden neurons and hidden layers.\relax }}{33}{figure.caption.38}\protected@file@percent }
\newlabel{fig:AccvsLrate3}{{37}{33}{Accuracy for different configurations of hidden neurons and hidden layers.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces  Accuracy for Ridge regression equivalent neural network for different learning rates and penalties using the sigmoid activation function.\relax }}{34}{figure.caption.39}\protected@file@percent }
\newlabel{fig:AccvsLrate4}{{38}{34}{Accuracy for Ridge regression equivalent neural network for different learning rates and penalties using the sigmoid activation function.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces  Accuracy for Ridge regression equivalent neural network for different learning rates and penalties using the RELU activation function.\relax }}{34}{figure.caption.40}\protected@file@percent }
\newlabel{fig:AccvsLrate5}{{39}{34}{Accuracy for Ridge regression equivalent neural network for different learning rates and penalties using the RELU activation function.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces  Accuracy for Ridge regression equivalent neural network for different hidden neuron/layer configurations using the sigmoid activation function.\relax }}{35}{figure.caption.41}\protected@file@percent }
\newlabel{fig:AccvsLrate6}{{40}{35}{Accuracy for Ridge regression equivalent neural network for different hidden neuron/layer configurations using the sigmoid activation function.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces  Accuracy for Ridge regression equivalent neural network for different hidden neuron/layer configurations using the RELU activation function.\relax }}{35}{figure.caption.42}\protected@file@percent }
\newlabel{fig:AccvsLrate7}{{41}{35}{Accuracy for Ridge regression equivalent neural network for different hidden neuron/layer configurations using the RELU activation function.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces  Accuracy for logistic SGD for both OLS and Ridge equivalent implementations using both the Scikit and my own SGD algorithm. The results from the Scikit implementation is dashed and same color lines represent models with same $L_2$ penalty.\relax }}{37}{figure.caption.43}\protected@file@percent }
\newlabel{fig:finalAcc1}{{42}{37}{Accuracy for logistic SGD for both OLS and Ridge equivalent implementations using both the Scikit and my own SGD algorithm. The results from the Scikit implementation is dashed and same color lines represent models with same $L_2$ penalty.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces  Accuracy for logistic SGD for both OLS and Ridge equivalent implementations using my own SGD algorithm.\relax }}{38}{figure.caption.44}\protected@file@percent }
\newlabel{fig:finalAcc2}{{43}{38}{Accuracy for logistic SGD for both OLS and Ridge equivalent implementations using my own SGD algorithm.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces  Accuracy for logistic SGD for both OLS and Ridge equivalent implementations using the Scikit SGD algorithm.\relax }}{38}{figure.caption.45}\protected@file@percent }
\newlabel{fig:finalAcc3}{{44}{38}{Accuracy for logistic SGD for both OLS and Ridge equivalent implementations using the Scikit SGD algorithm.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces  Accuracy as function of both learning rate and penalty for my own implementation.\relax }}{39}{figure.caption.46}\protected@file@percent }
\newlabel{fig:finalAcc4}{{45}{39}{Accuracy as function of both learning rate and penalty for my own implementation.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces  Accuracy as function of both learning rate and penalty for the Scikit implementation.\relax }}{40}{figure.caption.47}\protected@file@percent }
\newlabel{fig:finalAcc5}{{46}{40}{Accuracy as function of both learning rate and penalty for the Scikit implementation.\relax }{figure.caption.47}{}}
